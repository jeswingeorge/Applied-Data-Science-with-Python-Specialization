{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.0** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-data-analysis/resources/0dhYG) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Distribution:__ Set of all possible random variables.\n",
    "\n",
    "When a coin is flipped it has a probability of landing heads up and a probability of landing tails up. If we flip a coin many times we collect a number of measurements of the heads and tails that landed face up and intuitively we know that the number of times we get a heads up will be equal about the number of times we get a tails up for a fair coin. If you flipped a coin a hundred times and you received heads each time you'd probably doubt the fairness of that coin. We can consider the result of each flip of this coin as a __random variable__. \n",
    "\n",
    "\n",
    "(When we can consider the set of all possible random variables together we call this a __distribution__.) In this case the distribution is called __binomial__ since there's two possible outputs a heads or a tails. It's also an example of a discreet distribution since there are only categories being used a heads and a tails and not real numbers. \n",
    "\n",
    "NumPy actually has some distributions built into it allowing us to make random flips of a coin with given parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we ask for a number from the NumPy binomial distribution. We have two parameters to pass in. The first is the number of times we want it to run. The second is the chance we get a zero, which we will use to represent heads here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we run the simulation a thousand times and divided the result by a thousand. Well you see a number pretty close to 0.5 which means half of the time we had a heads and half of the time we had a tails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.515"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(1000, 0.5)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Q) Suppose we want to simulate the probability of flipping a fair coin 20 times, and getting a number greater than or equal to 15. Use np.random.binomial(n, p, size) to do 10000 simulations of flipping a fair coin 20 times, then see what proportion of the simulations are 15 or greater.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0203\n"
     ]
    }
   ],
   "source": [
    "x = np.random.binomial(20, .5, 10000)\n",
    "print((x>=15).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course an even weighted binomial distribution is only one simple example. We can also have unevenly weighted binomial distributions. For instance what's the chance although we're tornado today while Iâ€™m filming. It's pretty low even though we do get tornadoes here. So maybe there a hundredth of a percentage chance. We can put this into a binomial distribution as a weighting in NumPy. If we run this 100,000 times we see there are pretty minimal tornado events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chance_of_tornado = 0.01/100\n",
    "np.random.binomial(100000, chance_of_tornado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the computational tools are starting to allow us to simulate the world which helps us answer questions. I could have shown you the math behind this so we could have worked out the probabilities. But a simulation is essentially another form of inquiry. Let's take one more example. Let's say the chance of a tornado here in Ann Arbor on any given day, is 1% regardless of the time of year. That's higher than realistic but it makes for a quicker demo. And lets say if there's a tornado I'm going to get away from the windows and hide, then come back and do my recording the next day. __So what's the chance of this happening two days in a row?__\n",
    "\n",
    "can use the binomial distribution in NumPy to simulate this.\n",
    "\n",
    "Here we create an empty list and we create a number of potential tornado events by asking the NumPy binomial function using our chance of tornado. We'll do this a million times which is just shy of 3,000 years worth of events.\n",
    "\n",
    "This process is called __sampling the distribution__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 tornadoes back to back in 2739.72602739726 years\n"
     ]
    }
   ],
   "source": [
    "chance_of_tornado = 0.01\n",
    "\n",
    "tornado_events = np.random.binomial(1, chance_of_tornado, 1000000)\n",
    "    \n",
    "two_days_in_a_row = 0\n",
    "for j in range(1,len(tornado_events)-1):\n",
    "    if tornado_events[j]==1 and tornado_events[j-1]==1:\n",
    "        two_days_in_a_row+=1\n",
    "\n",
    "print('{} tornadoes back to back in {} years'.format(two_days_in_a_row, 1000000/365))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "point here though is that modern computational power allows us to very quickly simulate the effects of different parameters in a distribution. Leading to a new way of understanding the problem. You don't have to work out all the math you can quite often simulate the problem instead and observe the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04940074406142636"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2386201094119984"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for standard deviation\n",
    "$$\\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\overline{x})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9791495987350588"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = np.random.normal(0.75,size=1000)\n",
    "\n",
    "np.sqrt(np.sum((np.mean(distribution)-distribution)**2)/len(distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9791495987350588"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.03233005198519079"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "stats.kurtosis(distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05066848726431662"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.skew(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch distributions and take a look at a distribution called the Chi Squared distribution, which is also quite commonly used in statistic. The __Chi Squared Distribution__ has only one parameter called the __degrees of freedom__. The degrees of freedom is closely related to the number of samples that you take from a normal population. It's important for significance testing. But what I would like you to observe, is that as the degrees of freedom increases, the shape of the Chi Squared distribution changes. In particular, the skew to the left begins to move towards the center. We can observe this through simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9252854704121998"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared_df2 = np.random.chisquare(2, size=10000)\n",
    "stats.skew(chi_squared_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2967676775891372"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared_df5 = np.random.chisquare(5, size=10000)\n",
    "stats.skew(chi_squared_df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A histogram with our plot with the two degrees of freedom is skewed much further to the left, while our plot with the five degrees of freedom is not as highly skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3120f81e48>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f311f353160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output = plt.hist([chi_squared_df2,chi_squared_df5], bins=50, histtype='step', \n",
    "                  label=['2 degrees of freedom','5 degrees of freedom'])\n",
    "plt.legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that a distribution is just a shape that describes the probability of a value being pulled when we sample a population. And NumPy and SciPy each have a number of different distributions built in for us to be able to sample from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference : [Alen Downey's Think stats 2ed book](http://greenteapress.com/thinkstats2/thinkstats2.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "- A hypothesis is a statement that we can test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('grades.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>assignment1_grade</th>\n",
       "      <th>assignment1_submission</th>\n",
       "      <th>assignment2_grade</th>\n",
       "      <th>assignment2_submission</th>\n",
       "      <th>assignment3_grade</th>\n",
       "      <th>assignment3_submission</th>\n",
       "      <th>assignment4_grade</th>\n",
       "      <th>assignment4_submission</th>\n",
       "      <th>assignment5_grade</th>\n",
       "      <th>assignment5_submission</th>\n",
       "      <th>assignment6_grade</th>\n",
       "      <th>assignment6_submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B73F2C11-70F0-E37D-8B10-1D20AFED50B1</td>\n",
       "      <td>92.733946</td>\n",
       "      <td>2015-11-02 06:55:34.282000000</td>\n",
       "      <td>83.030552</td>\n",
       "      <td>2015-11-09 02:22:58.938000000</td>\n",
       "      <td>67.164441</td>\n",
       "      <td>2015-11-12 08:58:33.998000000</td>\n",
       "      <td>53.011553</td>\n",
       "      <td>2015-11-16 01:21:24.663000000</td>\n",
       "      <td>47.710398</td>\n",
       "      <td>2015-11-20 13:24:59.692000000</td>\n",
       "      <td>38.168318</td>\n",
       "      <td>2015-11-22 18:31:15.934000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98A0FAE0-A19A-13D2-4BB5-CFBFD94031D1</td>\n",
       "      <td>86.790821</td>\n",
       "      <td>2015-11-29 14:57:44.429000000</td>\n",
       "      <td>86.290821</td>\n",
       "      <td>2015-12-06 17:41:18.449000000</td>\n",
       "      <td>69.772657</td>\n",
       "      <td>2015-12-10 08:54:55.904000000</td>\n",
       "      <td>55.098125</td>\n",
       "      <td>2015-12-13 17:32:30.941000000</td>\n",
       "      <td>49.588313</td>\n",
       "      <td>2015-12-19 23:26:39.285000000</td>\n",
       "      <td>44.629482</td>\n",
       "      <td>2015-12-21 17:07:24.275000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D0F62040-CEB0-904C-F563-2F8620916C4E</td>\n",
       "      <td>85.512541</td>\n",
       "      <td>2016-01-09 05:36:02.389000000</td>\n",
       "      <td>85.512541</td>\n",
       "      <td>2016-01-09 06:39:44.416000000</td>\n",
       "      <td>68.410033</td>\n",
       "      <td>2016-01-15 20:22:45.882000000</td>\n",
       "      <td>54.728026</td>\n",
       "      <td>2016-01-11 12:41:50.749000000</td>\n",
       "      <td>49.255224</td>\n",
       "      <td>2016-01-11 17:31:12.489000000</td>\n",
       "      <td>44.329701</td>\n",
       "      <td>2016-01-17 16:24:42.765000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FFDF2B2C-F514-EF7F-6538-A6A53518E9DC</td>\n",
       "      <td>86.030665</td>\n",
       "      <td>2016-04-30 06:50:39.801000000</td>\n",
       "      <td>68.824532</td>\n",
       "      <td>2016-04-30 17:20:38.727000000</td>\n",
       "      <td>61.942079</td>\n",
       "      <td>2016-05-12 07:47:16.326000000</td>\n",
       "      <td>49.553663</td>\n",
       "      <td>2016-05-07 16:09:20.485000000</td>\n",
       "      <td>49.553663</td>\n",
       "      <td>2016-05-24 12:51:18.016000000</td>\n",
       "      <td>44.598297</td>\n",
       "      <td>2016-05-26 08:09:12.058000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5ECBEEB6-F1CE-80AE-3164-E45E99473FB4</td>\n",
       "      <td>64.813800</td>\n",
       "      <td>2015-12-13 17:06:10.750000000</td>\n",
       "      <td>51.491040</td>\n",
       "      <td>2015-12-14 12:25:12.056000000</td>\n",
       "      <td>41.932832</td>\n",
       "      <td>2015-12-29 14:25:22.594000000</td>\n",
       "      <td>36.929549</td>\n",
       "      <td>2015-12-28 01:29:55.901000000</td>\n",
       "      <td>33.236594</td>\n",
       "      <td>2015-12-29 14:46:06.628000000</td>\n",
       "      <td>33.236594</td>\n",
       "      <td>2016-01-05 01:06:59.546000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             student_id  assignment1_grade  \\\n",
       "0  B73F2C11-70F0-E37D-8B10-1D20AFED50B1          92.733946   \n",
       "1  98A0FAE0-A19A-13D2-4BB5-CFBFD94031D1          86.790821   \n",
       "2  D0F62040-CEB0-904C-F563-2F8620916C4E          85.512541   \n",
       "3  FFDF2B2C-F514-EF7F-6538-A6A53518E9DC          86.030665   \n",
       "4  5ECBEEB6-F1CE-80AE-3164-E45E99473FB4          64.813800   \n",
       "\n",
       "          assignment1_submission  assignment2_grade  \\\n",
       "0  2015-11-02 06:55:34.282000000          83.030552   \n",
       "1  2015-11-29 14:57:44.429000000          86.290821   \n",
       "2  2016-01-09 05:36:02.389000000          85.512541   \n",
       "3  2016-04-30 06:50:39.801000000          68.824532   \n",
       "4  2015-12-13 17:06:10.750000000          51.491040   \n",
       "\n",
       "          assignment2_submission  assignment3_grade  \\\n",
       "0  2015-11-09 02:22:58.938000000          67.164441   \n",
       "1  2015-12-06 17:41:18.449000000          69.772657   \n",
       "2  2016-01-09 06:39:44.416000000          68.410033   \n",
       "3  2016-04-30 17:20:38.727000000          61.942079   \n",
       "4  2015-12-14 12:25:12.056000000          41.932832   \n",
       "\n",
       "          assignment3_submission  assignment4_grade  \\\n",
       "0  2015-11-12 08:58:33.998000000          53.011553   \n",
       "1  2015-12-10 08:54:55.904000000          55.098125   \n",
       "2  2016-01-15 20:22:45.882000000          54.728026   \n",
       "3  2016-05-12 07:47:16.326000000          49.553663   \n",
       "4  2015-12-29 14:25:22.594000000          36.929549   \n",
       "\n",
       "          assignment4_submission  assignment5_grade  \\\n",
       "0  2015-11-16 01:21:24.663000000          47.710398   \n",
       "1  2015-12-13 17:32:30.941000000          49.588313   \n",
       "2  2016-01-11 12:41:50.749000000          49.255224   \n",
       "3  2016-05-07 16:09:20.485000000          49.553663   \n",
       "4  2015-12-28 01:29:55.901000000          33.236594   \n",
       "\n",
       "          assignment5_submission  assignment6_grade  \\\n",
       "0  2015-11-20 13:24:59.692000000          38.168318   \n",
       "1  2015-12-19 23:26:39.285000000          44.629482   \n",
       "2  2016-01-11 17:31:12.489000000          44.329701   \n",
       "3  2016-05-24 12:51:18.016000000          44.598297   \n",
       "4  2015-12-29 14:46:06.628000000          33.236594   \n",
       "\n",
       "          assignment6_submission  \n",
       "0  2015-11-22 18:31:15.934000000  \n",
       "1  2015-12-21 17:07:24.275000000  \n",
       "2  2016-01-17 16:24:42.765000000  \n",
       "3  2016-05-26 08:09:12.058000000  \n",
       "4  2016-01-05 01:06:59.546000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2315"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early = df[df['assignment1_submission'] <= '2015-12-31']\n",
    "late = df[df['assignment1_submission'] > '2015-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assignment1_grade    74.972741\n",
       "assignment2_grade    67.252190\n",
       "assignment3_grade    61.129050\n",
       "assignment4_grade    54.157620\n",
       "assignment5_grade    48.634643\n",
       "assignment6_grade    43.838980\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assignment1_grade    74.017429\n",
       "assignment2_grade    66.370822\n",
       "assignment3_grade    60.023244\n",
       "assignment4_grade    54.058138\n",
       "assignment5_grade    48.599402\n",
       "assignment6_grade    43.844384\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing hypothesis testing, we have to choose a significance level as a threshold for how much of a chance we're willing to accept. This significance level is typically called alpha. It can vary greatly, depending on what you're going to do with the result and the amount of noise you expect in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.ttest_ind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.400549944897566, pvalue=0.16148283016060577)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(early['assignment1_grade'], late['assignment1_grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SciPy library contains a number of different statistical tests and forms a basis for hypothesis testing in Python. A T test is one way to compare the means of two different populations. In the SciPy library, the T test end function will compare two independent samples to see if they have different means. I'm not going to go into the details of any of this statistical test here. But instead, we'd recommend that you check out the Wikipedia page on particular test or consider taking a full statistics course if this is unfamiliar to you. But I do want to note that most statistical tests expect that the data conforms to a certain distribution, a shape. So, you shouldn't apply such tests blindly and should investigate your data first. If we want to compare the assignment grades for the first assignment between the two populations, we could generate a T test by passing these two series into the T test in function. The result is a two with a test statistic and a p-value. The p-value here is much larger than our 0.05. So we cannot reject the null hypothesis, which is that the two populations are the same. In more late terms, we would say that there's no statistically significant difference between these two sample means. Let's check with assignment two grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.3239868220912567, pvalue=0.18563824610067967)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(early['assignment2_grade'], late['assignment2_grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.7116160037010733, pvalue=0.087101516341556676)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(early['assignment3_grade'], late['assignment3_grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's much closer, but still beyond our threshold value. It's important to stop here and talk about serious process from with how we're handling this investigation of the difference between these two populations. When we set the alpha to be 0.05, we're saying that we expect it that there will be positive result, 5% of the time just to the chance.\n",
    "\n",
    "As we run more and more T tests, we're more likely to find a positive result just because of the number of T tests we have run.\n",
    "\n",
    "When a data scientist run many tests in this way, it's called p-hacking or dredging and it's a serious methodological issue. P-hacking results in spurious correlations instead of generalizable results. There are a couple of different ways you can deal with p-hacking. The first is called the Bonferroni correction. In this case, you simply tighten your alpha value, the threshold of significance based on the number of tests you're running. So if you choose 0.05 with 1 test, you want to run 3 test, you reduce alpha by multiplying 0.05 by one-third to get a new value of 0.01 sub. I personally find this approach to be very conservative. Another option is to hold out some of your data for testing to see how generizable your result is. In this case, we might take half of our data for each of the two data frames, run our T test with that. Form specific hypothesis based on the result of these tests, then run very limited tests on the rest of the data.\n",
    "\n",
    "This method is actually heavily used in machine learning when building predictive models where it's called cross fold validation and you'll learn more about this in third course in this specialization.\n",
    "\n",
    "A final method which has come about is the pre-registration of your experiment.\n",
    "\n",
    "In this step, you would outline what you expect to find and why, and describe the test that would backup a positive proof of this. You register it with a third party, in academic circles, this is often a journal who determines whether it's a reasonable test or rather not.\n",
    "\n",
    "You then run your study and report the results, regardless as to whether they were positive or not. Here, there is a larger burden on connecting to existing theory since you need to convince reviewers that the experiment is likely to test fully a given hypothesis. In this lecture, we've discussed just some of the basics of hypothesis testing in Python. I introduced you to the SciPy library, which you can use for T testing. We've discussed some of the practical issues which arise from looking for statistical significance. There's much more to learn about hypothesis testing. For instance, there are different tests used, depending on the shape of your data and different ways to report results instead of just p-values such as confidence intervals. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
